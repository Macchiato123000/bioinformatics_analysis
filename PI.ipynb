{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from Bio import SeqIO\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Read sequence from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizeSeq(fileName):\n",
    "    file = SeqIO.parse(open(fileName),'fasta')\n",
    "    oneHot = []\n",
    "    numLable = []\n",
    "    seq_name = []\n",
    "    seq_gene = []\n",
    "    i = 0\n",
    "    for seq in file:\n",
    "        seq_name.append(seq.id)\n",
    "        seq_gene.append(str(seq.seq))\n",
    "    \n",
    "        #One-hot Encoding\n",
    "        oneHotDict = {'A': np.array([1,0,0,0]),\n",
    "                      'C': np.array([0,1,0,0]),\n",
    "                      'G': np.array([0,0,1,0]),\n",
    "                      'T': np.array([0,0,0,1]),\n",
    "                      'N': np.array([0,0,0,0])} # may need change\n",
    "\n",
    "        oneHot.append(np.array(list(map(lambda letter: oneHotDict[letter], seq_gene[i])))\n",
    "                        )\n",
    "        numLableDict = {'A': np.array([0]),\n",
    "                        'C': np.array([1]),\n",
    "                        'G': np.array([2]),\n",
    "                        'T': np.array([3]),\n",
    "                        'N': np.array([4])} # may need change\n",
    "\n",
    "        numLable.append(np.array(list(map(lambda letter: numLableDict[letter], seq_gene[i])))\n",
    "                        )\n",
    "        i+=1\n",
    "    return np.array(oneHot), np.array(numLable), seq_gene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_onehot,nagative_num,nagative_seq = vectorizeSeq(\"promoter_800_900.fa\")\n",
    "positive_onehot,positive_num,positive_seq = vectorizeSeq(\"promoter_-50_50.fa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16455, 101, 4)\n",
      "(16455, 101, 4)\n"
     ]
    }
   ],
   "source": [
    "#print(promoter[0])\n",
    "#print(promoter_onehot[0])\n",
    "#print(promoter_num[0])\n",
    "\n",
    "print(positive_onehot.shape)\n",
    "print(negative_onehot.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Creating Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lableGenerator(positive, nagative):\n",
    "    y = np.array([1, 0])\n",
    "    y = np.repeat(y, [positive.shape[0], nagative.shape[0]])\n",
    "    y = to_categorical(y)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Creating dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = lableGenerator(positive_onehot, negative_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PromoterDataset(positive, negative):\n",
    "    return np.concatenate((positive, negative), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = PromoterDataset(positive_onehot, negative_onehot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will split the whole dataset into:\n",
    "* 60% for training\n",
    "* 20% for validation\n",
    "* 20% for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices100 = np.arange(x.shape[0])\n",
    "X100_train, X100_test, y100_train, y100_test, idx100_train, idx100_test = train_test_split(x, y, indices100, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.models import model_from_yaml\n",
    "from keras.layers.convolutional import Conv1D # Keras 2 syntax\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Activation\n",
    "from keras.initializers import Constant\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "#from keras.callbacks import TensorBoard\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN1(length, dimensions, nb_filters, kernel_size, pool_size):\n",
    "    # Initialize network\n",
    "    classifier = Sequential()\n",
    "    \n",
    "    # Layers 1 and 2: Add convolutional layers\n",
    "    classifier.add(Conv1D(filters=nb_filters, kernel_size=kernel_size, input_shape=(length, dimensions), activation=\"relu\"))\n",
    "    \n",
    "    # Layer 3: Max-Pool // may change to average pool\n",
    "    classifier.add(MaxPooling1D(pool_size = pool_size))\n",
    "    \n",
    "    # Layer 7: Flatten to be fed into fully-connected layers\n",
    "    classifier.add(Flatten())\n",
    "    \n",
    "    # Layers 8: fully connected layer with 256 nodes\n",
    "    classifier.add(Dense(units = 128, activation = 'relu'))\n",
    "    \n",
    "    # Layer 11: final layer for binary prediction using softmax activation function\n",
    "    classifier.add(Dense(units = 2, activation='softmax'))\n",
    "    \n",
    "    print(classifier.summary())\n",
    "    # Return compiled network\n",
    "    return classifier\n",
    "\n",
    "#classifier = CNN1(251, 4, 300, 21, 231)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 81, 300)           25500     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 1, 300)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               38528     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 64,286\n",
      "Trainable params: 64,286\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "classifier = CNN1(101, 4, 300, 21, 81)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_history(model_history):\n",
    "    fig, axs = plt.subplots(1,2,figsize=(15,5))\n",
    "    \n",
    "    # summarize history for training accuracy, validation accuracy, and loss\n",
    "    axs[0].plot(range(1,len(model_history.history['acc'])+1),model_history.history['acc'])\n",
    "    axs[0].plot(range(1,len(model_history.history['val_acc'])+1),model_history.history['val_acc'])\n",
    "    \n",
    "    # Set titles/labels, labels, etc.\n",
    "    axs[0].set_title('Model Accuracy')\n",
    "    axs[0].set_ylabel('Accuracy')\n",
    "    axs[0].set_xlabel('Epoch')\n",
    "    axs[0].set_xticks(np.arange(1,len(model_history.history['acc'])+1),len(model_history.history['acc'])/10)\n",
    "    axs[0].legend(['train', 'val'], loc='best')\n",
    "    \n",
    "    # summarize history for loss\n",
    "    axs[1].plot(range(1,len(model_history.history['loss'])+1),model_history.history['loss'])\n",
    "    axs[1].plot(range(1,len(model_history.history['val_loss'])+1),model_history.history['val_loss'])\n",
    "    axs[1].set_title('Model Loss')\n",
    "    axs[1].set_ylabel('Loss')\n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    axs[1].set_xticks(np.arange(1,len(model_history.history['loss'])+1),len(model_history.history['loss'])/10)\n",
    "    axs[1].legend(['train', 'val'], loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "earlystop = EarlyStopping(monitor='val_acc', min_delta=0.0001, patience=5, \\\n",
    "                          verbose=1, mode='auto')\n",
    "\n",
    "callbacks_list = [earlystop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 The Adama optimizer and binary cross-entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile neural network\n",
    "classifier.compile(loss='binary_crossentropy', # Cross-entropy\n",
    "                   optimizer='adam',\n",
    "                   metrics=['accuracy']) # Accuracy performance metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Run on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19746 samples, validate on 6582 samples\n",
      "Epoch 1/30\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "start = time.time()\n",
    "\n",
    "model_info = classifier.fit(X100_train, y100_train, verbose=1, \\\n",
    "                            validation_steps = 0, \\\n",
    "                            validation_split = 0.25, \\\n",
    "                            batch_size = 16, \\\n",
    "                            callbacks = callbacks_list, \\\n",
    "                            epochs = 30)\n",
    "\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_history(model_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute test accuracy\n",
    "print(\"Accuracy on test data is: {0}\".format(classifier.evaluate(X100_test, y100_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X100_test)\n",
    "rounded = [round(val[1]) for val in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.evaluate(X100_test, y100_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y100_test[:,0], rounded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y100_test[:,1], rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
